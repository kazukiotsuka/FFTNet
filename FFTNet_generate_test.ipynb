{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/home/kaz/miniconda3/envs/core/lib/python36.zip', '/home/kaz/miniconda3/envs/core/lib/python3.6', '/home/kaz/miniconda3/envs/core/lib/python3.6/lib-dynload', '/home/kaz/miniconda3/envs/core/lib/python3.6/site-packages', '/home/kaz/miniconda3/envs/core/lib/python3.6/site-packages/warpctc_pytorch-0.1-py3.6-linux-x86_64.egg', '/home/kaz/miniconda3/envs/core/lib/python3.6/site-packages/setuptools-27.2.0-py3.6.egg', '/home/kaz/miniconda3/envs/core/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg', '/home/kaz/miniconda3/envs/core/lib/python3.6/site-packages/IPython/extensions', '/home/kaz/.ipython', '../../']\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "from IPython.core.debugger import Pdb\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "import pickle\n",
    "sys.path.append('../../')\n",
    "print(sys.path)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import librosa\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from models.mlmodeldic import MLModelDic\n",
    "from models.phoneme import Phoneme43\n",
    "from tqdm import tqdm\n",
    "from mlutils.utils import time_elapse, time_since, update_lr, shape, calc_acc, plot\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFTNetQueue(object):\n",
    "    def __init__(self, batch_size, n_channels, input_size, is_cuda=False):\n",
    "        super(FFTNetQueue, self).__init__()\n",
    "        self.__batch_size__ = batch_size\n",
    "        self.__n_channels__ = n_channels\n",
    "        self.__input_size__ = input_size\n",
    "        self.__is_cuda__ = is_cuda\n",
    "        self.queue = []\n",
    "        self.reset(batch_size, is_cuda)\n",
    "    \n",
    "    def reset(self, batch_size, is_cuda=False):\n",
    "        self.queue = torch.zeros([batch_size, self.__n_channels__, self.__input_size__])\n",
    "        if is_cuda:\n",
    "            self.queue = self.queue.cuda()\n",
    "            \n",
    "    def enqueue(self, sample_to_push):\n",
    "        \"\"\"Return the last sample and insert the new sample to the end of queue.\n",
    "        \n",
    "        args:\n",
    "            - sample_to_push: (B, C, T=1)\n",
    "        \n",
    "        1. remove first, 2. push sample_to_push to last, 3. return z\n",
    "        [0, 1, 2, 3, .., y, z] -> [1, 2, 3, .., y, z, sample_to_push]\n",
    "        \"\"\"\n",
    "        Pdb().set_trace()\n",
    "        sample_to_pop = self.queue[:, :, -1:].data  # get last\n",
    "        self.queue[:, :, :-1] = self.queue[:, :, 1:]  # [0, 1, 2, 3, .., y, z] -> [1, 2, 3, .., y, z, z]\n",
    "        return sample_to_pop  # return the last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFTLayer(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, layer_idx:int, input_size: int):\n",
    "        super().__init__()\n",
    "        # settings\n",
    "        self.__in_channels__ = in_channels\n",
    "        self.__out_channels__ = out_channels\n",
    "        #self.__f0_channels__ = f0_channels  # always 1 because only the first layer\n",
    "        #self.__phonemes_channels__ = phonemes_channels   # always 1 because only the first layer\n",
    "        self.__input_size__ = input_size  # input size for this layer (=depth**2)\n",
    "        self.__layer_idx__ = layer_idx\n",
    "        # generate queues\n",
    "        self.sample_queue = None\n",
    "        self.f0_queue = None\n",
    "        self.phonemes_queue = None\n",
    "        #self.f0_buffer = None\n",
    "        #self.phonemes_buffer = None\n",
    "        # layers\n",
    "        self.conv1d_L = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "        self.conv1d_R = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "        if layer_idx == 0:\n",
    "            self.conv1d_f0_L = nn.Conv1d(1, out_channels, kernel_size=1)  #nn.Conv1d(f0_channels, out_channels, kernel_size=1)\n",
    "            self.conv1d_f0_R = nn.Conv1d(1, out_channels, kernel_size=1)  #nn.Conv1d(f0_channels, out_channels, kernel_size=1)\n",
    "            self.conv1d_phonemes_L = nn.Conv1d(1, out_channels, kernel_size=1)  #nn.Conv1d(phonemes_channels, out_channels, kernel_size=1)\n",
    "            self.conv1d_phonemes_R = nn.Conv1d(1, out_channels, kernel_size=1)  #nn.Conv1d(phonemes_channels, out_channels, kernel_size=1)\n",
    "        self.conv1d_out = nn.Conv1d(out_channels, out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x, f0=None, phonemes=None):\n",
    "        #Pdb().set_trace()\n",
    "        x_L = self.conv1d_L(x[:, :, :-self.__input_size__//2])\n",
    "        x_R = self.conv1d_R(x[:, :, self.__input_size__//2:])\n",
    "        if f0 is not None and phonemes is not None:\n",
    "            # only called in the first layer\n",
    "            f0_L = self.conv1d_f0_L(f0[:, :, :-self.__input_size__//2])\n",
    "            f0_R = self.conv1d_f0_R(f0[:, :, self.__input_size__//2:])\n",
    "            # NOTE: categorical sequencial data works for convnets, but works without any spatial encoding?\n",
    "            phonemes_L = self.conv1d_phonemes_L(phonemes[:, :, :-self.__input_size__//2])\n",
    "            phonemes_R = self.conv1d_phonemes_R(phonemes[:, :, self.__input_size__//2:])\n",
    "            z = x_L + x_R + f0_L + f0_R + phonemes_L + phonemes_R\n",
    "        else:\n",
    "            z = x_L + x_R\n",
    "        x = F.relu(z)\n",
    "        return F.relu(self.conv1d_out(x))\n",
    "    \n",
    "    def generate_step(self, x, f0=None, phoneme=None):\n",
    "        # 最初のstepで入力されるのはx=randint(1,256,(1,1,1))\n",
    "        #Pdb().set_trace()\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # init queues\n",
    "        if self.sample_queue is None:\n",
    "            self.sample_queue = FFTNetQueue(B, self.__in_channels__, self.__input_size__, x.is_cuda)\n",
    "        if (self.f0_queue is None and f0 is not None) and (self.phonemes_queue is None and phoneme is not None):\n",
    "            self.f0_queue = FFTNetQueue(B, self.__in_channels__, self.__input_size__, x.is_cuda)\n",
    "            self.phonemes_queue = FFTNetQueue(B, self.__in_channels__, self.__input_size__, x.is_cuda)\n",
    "            \n",
    "        # input - current sample (L) / previous sample (R)\n",
    "        x_L = x\n",
    "        x_R = self.sample_queue.enqueue(x)  # (B, T+C) (enqueueされるのはbufferの末の要素のみ)\n",
    "        # forward to conv1d\n",
    "        z1 = self.conv1d_L(x_L)\n",
    "        z2 = self.conv1d_R(x_R)\n",
    "        z = z1 + z2\n",
    "        \n",
    "        # f0, phonemes\n",
    "        if f0 is not None and phoneme is not None:\n",
    "            f0 = torch.FloatTensor([f0]).view(1,1,1)\n",
    "            phoneme = torch.FloatTensor([phoneme]).view(1,1,1)\n",
    "            f0_L = f0\n",
    "            f0_R = self.f0_queue.enqueue(f0)\n",
    "            phoneme_L = phoneme\n",
    "            phoneme_R = self.phonemes_queue.enqueue(phoneme)\n",
    "            # forward to conv1d\n",
    "            z_f0_1 = self.conv1d_f0_L(f0_L)\n",
    "            z_f0_2 = self.conv1d_f0_R(f0_R)\n",
    "            z_phoneme_1 = self.conv1d_phonemes_L(phoneme_L)\n",
    "            z_phoneme_2 = self.conv1d_phonemes_R(phoneme_R)\n",
    "            z = z + z_f0_1 + z_f0_2 + z_phoneme_1 + z_phoneme_2\n",
    "            \n",
    "        z = F.relu(z)\n",
    "        z = F.relu(self.conv1d_out(z))\n",
    "        z = z.view(B, -1, 1)\n",
    "        return z  # (B, 1, 1)\n",
    "\n",
    "    #def generate_step(self, x, f0=None, phonemes=None):\n",
    "    #    \"\"\"\n",
    "    #    >>> x\n",
    "    #    tensor([-1.0000, -1.0000, -1.0000,  0.1000,  0.2000,  0.3000,  0.4000,  0.5000,\n",
    "    #             0.6000,  0.7000,  0.8000])\n",
    "    #    >>> F.relu(x)\n",
    "    #    tensor([0.0000, 0.0000, 0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000,\n",
    "    #            0.7000, 0.8000])\n",
    "    #    >>> conv = torch.nn.Conv1d(len(x), out_channels=1, kernel_size=1)\n",
    "    #    >>> conv(x.view(1,-1,1))\n",
    "    #    tensor([[[0.4139]]], grad_fn=<SqueezeBackward1>)\n",
    "    #    >>> F.relu(conv(x.view(1,-1,1)))\n",
    "    #    tensor([[[0.4139]]], grad_fn=<ReluBackward>)\n",
    "    #    \"\"\"\n",
    "    #    Pdb().set_trace()\n",
    "    #    # MEMO: why only the first and the last?\n",
    "    #    # mozillaの実装ではLにx.view(:, -1) (B, T=1), Rにbufferの末要素(B, T=1)を入力している\n",
    "    #    x_L = self.conv1d_L(x[:, :, :1])  # x(t=0): (1, 1, 1025), x[:, :, :1](t=0) (1,1,1), x_L(t=0): (1, 256, 1)\n",
    "    #    x_R = self.conv1d_R(x[:, :, -1:])  # x(t=0): (1, 1, 1025), x[:, :, :1](t=0) (1,1,1), x_R(t=0): (1, 256, 1)\n",
    "#\n",
    "    #    if f0 and phonemes:\n",
    "    #        # list to tensor (B:1, C:1, T:len(f0))\n",
    "    #        f0 = torch.FloatTensor(f0).view(1, 1, -1)  # f0 (1, 1, 16000) when seq_len is 16000\n",
    "    #        phonemes = torch.FloatTensor(phonemes).view(1, 1, -1) \n",
    "    #        # forward\n",
    "    #        f0_L = self.conv1d_f0_L(f0[:, :, :-self.__input_size__//2])  # f0[:, :, :-self.__input_size__//2] (1, 1, 14976), f0_L: (1, 256, 14976)  when seq_len is 16000\n",
    "    #        f0_R = self.conv1d_f0_R(f0[:, :, self.__input_size__//2:])  # f0[:, :, self.__input_size__//2:] (1, 1, 14976), f0_R: (1, 256, 14976)  when seq_len is 16000\n",
    "    #        # NOTE: categorical sequencial data works for convnets, but works without any spatial encoding?\n",
    "    #        phonemes_L = self.conv1d_phonemes_L(phonemes[:, :, :-self.__input_size__//2])\n",
    "    #        phonemes_R = self.conv1d_phonemes_R(phonemes[:, :, self.__input_size__//2:])\n",
    "    #        x = x_L + x_R + f0_L + f0_R + phonemes_L + phonemes_R  # x_L: (1, 256, 1) x_R: (1, 256, 1)\n",
    "    #    else:\n",
    "    #        x = x_L + x_R\n",
    "    #        \n",
    "    #    # NOTE: t=0でbuffer size(1, 256, 513)と一致しているはず?\n",
    "    #    x = F.relu(x)  # x(t=0): (1, 256, 14976) when seq_len is 16000\n",
    "    #    x = F.relu(self.conv1d_out(x))  # x(t=0): (1, 256, 14976) when seq_len is 16000\n",
    "    #    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFTNet(nn.Module):\n",
    "    def __init__(self, n_channels=256, n_depth=11, n_classes=256, disable_cuda=False):\n",
    "        super().__init__()\n",
    "        self.__disable_cuda__ = disable_cuda\n",
    "        self.__n_channels__ = n_channels\n",
    "        #self.__f0_channels__ = f0_channels\n",
    "        #self.__phonemes_channels__ = phonemes_channels\n",
    "        self.__n_depth__ = n_depth\n",
    "        self.__n_classes__ = n_classes  # 256 categorical μ-law encoding\n",
    "        self.__receptive_field_size__ = 2**n_depth  # 2**11 = 2048\n",
    "        fft_layers = []\n",
    "        for idx, N in enumerate([2**i for i in range(n_depth, 0, -1)]):\n",
    "            if idx == 0:\n",
    "                fft_layers += [FFTLayer(1, n_channels, idx, N)]\n",
    "            else:\n",
    "                fft_layers += [FFTLayer(n_channels, n_channels, idx, N)]\n",
    "        self.fft_layers = nn.ModuleList(fft_layers)\n",
    "        self.fully_connected = nn.Linear(n_channels, n_classes)\n",
    "        print(f'Receptive Field: {self.__receptive_field_size__} samples')\n",
    "        self.num_params(self)\n",
    "        \n",
    "    def settings(self) -> dict:\n",
    "        return {\n",
    "            'n_channels': self.__n_channels__,\n",
    "            'n_depth': self.__n_depth__,\n",
    "            'n_classes': self.__n_classes__,\n",
    "            'receptive_field_size': self.__receptive_field_size__\n",
    "        }\n",
    "    \n",
    "    def settings_str(self) -> str:\n",
    "        return f'channels{self.__n_channels__}_depth{self.__n_depth__}_class{self.__n_classes__}_rfsize{self.__receptive_field_size__}'\n",
    "        \n",
    "    def num_params(self, model) :\n",
    "        parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "        parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000\n",
    "        print('Trainable Parameters: %.3f million' % parameters)\n",
    "        \n",
    "    def forward(self, x, f0=None, phonemes=None):\n",
    "        #Pdb().set_trace()\n",
    "        B, C, T = x.size()\n",
    "        padding = torch.zeros(B, C, self.__receptive_field_size__)\n",
    "        if not self.__disable_cuda__:\n",
    "            padding = padding.cuda()\n",
    "        x = torch.cat([padding, x], dim=-1)\n",
    "        f0 = torch.cat([padding, f0], dim=-1)\n",
    "        phonemes = torch.cat([padding, phonemes], dim=-1)\n",
    "        _x = x\n",
    "        for i, fft_layer in enumerate(self.fft_layers):\n",
    "            if i == 0:\n",
    "                _x = fft_layer(_x, f0=f0, phonemes=phonemes)\n",
    "            else:\n",
    "                _x = fft_layer(_x)\n",
    "        _x = self.fully_connected(_x.transpose(1, 2))\n",
    "        return F.log_softmax(_x, dim=-1)\n",
    "    \n",
    "    def padding(self, x):\n",
    "        B, C, T = x.size()\n",
    "        max_len = np.max([d.shape[0] - 1 for d in x])\n",
    "        x_pad = np.zeros([B, self.__receptive_field_size__ + max_len])\n",
    "        for i, signal in enumerate(x):\n",
    "            pad_w = max_len - signal.shape[0]\n",
    "        # pad left with receptive field and right with max_len in the batch\n",
    "        wav = np.pad(wav, [self.__receptive_field_size__ - 1, pad_w], \n",
    "                        mode='constant', constant_values=0.0)\n",
    "        mel = np.pad(mel, [[self.__receptive_field_size__ - 1, pad_w], [0, 0]], \n",
    "                        mode='constant', constant_values=0.0)\n",
    "        \n",
    "    # queues are initialized in the first generate_step in each layer if queue is None\n",
    "    #def init_queues(self, batch_size):\n",
    "    #    for i, f in enumerate(self.fft_layers):\n",
    "    #        # init each layers queue with zeros(B, C, field size)\n",
    "    #        f.init_queue(batch_size, self.__disable_cuda__)\n",
    "        \n",
    "    #def init_buffer(self):\n",
    "    #    \"\"\"Initialize weight buffer for each layer.\n",
    "    #    \n",
    "    #    set [\n",
    "    #        rand(1, 1, input size of layer 0),\n",
    "    #        rand(1, 1, input size of layer 1),\n",
    "    #        ...\n",
    "    #    ] to self.buffer\n",
    "    #    \n",
    "    #    \n",
    "    #    ipdb> p self.signal_buffer[0].shape\n",
    "    #    torch.Size([1, 1, 1025])\n",
    "    #    ipdb> p self.signal_buffer[1].shape\n",
    "    #    torch.Size([1, 256, 513])\n",
    "    #    ipdb> p self.signal_buffer[2].shape\n",
    "    #    torch.Size([1, 256, 257])\n",
    "    #    ipdb> p self.signal_buffer[3].shape\n",
    "    #    torch.Size([1, 256, 129])\n",
    "    #    ipdb> p self.signal_buffer[4].shape\n",
    "    #    torch.Size([1, 256, 65])\n",
    "    #    ipdb> p self.signal_buffer[5].shape\n",
    "    #    torch.Size([1, 256, 33])\n",
    "    #    ipdb> p self.signal_buffer[10].shape\n",
    "    #    torch.Size([1, 256, 2])\n",
    "    #    ipdb> p self.signal_buffer[11].shape\n",
    "    #    torch.Size([1, 256, 1])\n",
    "    #    \"\"\"\n",
    "    #    Pdb().set_trace()\n",
    "    #    # MEMO: mozillaの実装では各レイヤーインスタンスが2次元のbuffer(B, T)をもつ.\n",
    "    #    # 長さ1の入力サンプルを末に連結しつつその前のstepの入力サンプルをpopする\n",
    "    #    x = torch.rand(1, 1, self.fft_layers[0].__input_size__)\n",
    "    #    f0 = torch.rand(1, 1, self.fft_layers[0].__input_size__)\n",
    "    #    phonemes = torch.rand(1, 1, self.fft_layers[0].__input_size__)\n",
    "    #    if not self.__disable_cuda__:\n",
    "    #        x = x.cuda()\n",
    "    #        f0 = f0.cuda()\n",
    "    #        phonemes = phonemes.cuda()\n",
    "    #    #self.signal_buffer = [x[:, :, self.fft_layers[0].__input_size__//2-1:]]  # self.signal_buffer[0]: (1, 1, 1025) <= 2048-(2048//2-1) = 2048-1023 = 1025 \n",
    "    #    self.signal_buffer = [x[:, :, self.fft_layers[0].__input_size__//2:]]  # half size of input field\n",
    "    #    for i, f in enumerate(self.fft_layers):\n",
    "    #        x_L = f.conv1d_L(x[:, :, :-f.__input_size__//2])  # x_L(i=0): (1, 1, 1024), x_L(i=1): (1, 256, 512)\n",
    "    #        x_R = f.conv1d_R(x[:, :, f.__input_size__//2:])  # x_R(i=0): (1, 1, 1024), x_R(i=1): (1, 256, 512)\n",
    "    #        x = x_L + x_R  # z(i=0): (1, 256, 1024) = x_L: (1, 256, 1024) + x_R: (1, 256, 1024), z(i=1): (1, 256, 512)\n",
    "    #        x = F.relu(x)  # z(i=0): (1, 256, 1024), z(i=1): (1, 256, 512)\n",
    "    #        x = F.relu(f.conv1d_out(x))  # z(i=0): (1, 256, 1024), z(i=1): (1, 256, 512)\n",
    "    #        self.signal_buffer += [x[:, :, f.__input_size__//4-1:]]  #  z[:, :, f.__input_size__//4-1:] (i=0) (1, 256, 513), z[:, :, f.__input_size__//4-1:] (i=1) (1, 256, 257)\n",
    "            \n",
    "    def push_to_signal_buffer(self, i, y):\n",
    "        self.signal_buffer[i] = torch.cat([self.signal_buffer[i], y], dim=-1)[:, :, 1:]\n",
    "\n",
    "    def push_to_f0_buffer(self, i, y):\n",
    "        self.f0_buffer[i] = torch.cat([self.f0_buffer[i], y], dim=-1)[:, :, 1:]\n",
    "\n",
    "    def push_to_phonemes_buffer(self, i, y):\n",
    "        self.phonemes_buffer[i] = torch.cat([self.phonemes_buffer[i], y], dim=-1)[:, :, 1:]\n",
    "        \n",
    "    def class2float(self, x) :\n",
    "        \"\"\"Convert 256 categorical class to -1.0 ~ 1.0 value.\n",
    "        \n",
    "        0 -> -1.0\n",
    "        1 -> -0.9921568627450981\n",
    "        2 -> -0.9843137254901961\n",
    "        ...\n",
    "        256 -> 1.0\n",
    "        \"\"\"\n",
    "        return 2 * x.float() / (self.__n_classes__ - 1) - 1\n",
    "    \n",
    "    def duration_to_seq_len(self, duration_ms: int, sample_rate=16000):\n",
    "        return int(16000/1000)*duration_ms\n",
    "    \n",
    "    def generate(self, duration_ms: int, phonemes=None, pitches=None, sample_rate=16000):\n",
    "        #Pdb().set_trace()\n",
    "        with torch.no_grad():\n",
    "            c = 2\n",
    "            B = 1\n",
    "            output = []\n",
    "            start = time.time()\n",
    "            seq_len = self.duration_to_seq_len(duration_ms)\n",
    "            for t in range(seq_len):\n",
    "                if t == 0:\n",
    "                    # start sequence with random input\n",
    "                    x = torch.randint(1, 256, (1,1,1))\n",
    "                for i, f in enumerate(self.fft_layers):\n",
    "                    if i == 0 and (pitches and phonemes):\n",
    "                        x = f.generate_step(x, f0=pitches[t], phoneme=phonemes[t])\n",
    "                    else:\n",
    "                        x = f.generate_step(x)\n",
    "                x = self.fully_connected(x.squeeze(-1))\n",
    "                posterior = F.softmax(c * x.view(-1), dim=0)\n",
    "                # TODO: 2.3.2 Conditional sampling\n",
    "                dist = torch.distributions.Categorical(posterior)\n",
    "                sample = self.class2float(dist.sample())\n",
    "                output.append(sample)\n",
    "                speed = (t + 1) / (time.time() - start)\n",
    "                print(f'generate class {dist.sample()} ({sample}): {t+1}/{seq_len}, Speed: {speed:.2f} samples/sec')\n",
    "                x = torch.FloatTensor([dist.sample()]).view(B, 1, 1)\n",
    "                #Pdb().set_trace()\n",
    "        return torch.stack(output).cpu().numpy()\n",
    "\n",
    "    #def generate(self, duration_ms: int, phonemes=[], pitches=[], sample_rate=16000):\n",
    "    #    def duration_to_seq_len(duration_ms: int, sample_rate=16000):\n",
    "    #        return int(16000/1000)*duration_ms\n",
    "    #    Pdb().set_trace()\n",
    "    #    with torch.no_grad():\n",
    "    #        c = 2\n",
    "    #        self.init_buffers()\n",
    "    #        output = []\n",
    "    #        start = time.time()\n",
    "    #        seq_len = duration_to_seq_len(duration_ms)\n",
    "    #        for t in range(seq_len):\n",
    "    #            for i, f in enumerate(self.fft_layers):\n",
    "    #                if i == 0:\n",
    "    #                    x = f.generate_step(self.signal_buffer[i], f0=pitches, phonemes=phonemes)\n",
    "    #                else:\n",
    "    #                    x = f.generate_step(self.signal_buffer[i])\n",
    "    #                self.push_to_signal_buffer(i + 1, x)\n",
    "    #            Pdb().set_trace()\n",
    "    #            x = self.fully_connected(x.squeeze(-1))\n",
    "    #            posterior = F.softmax(c * x.view(-1), dim=0)\n",
    "    #            # TODO: 2.3.2 Conditional sampling\n",
    "    #            dist = torch.distributions.Categorical(posterior)\n",
    "    #            sample = self.class2float(dist.sample())\n",
    "    #            output.append(sample)\n",
    "    #            self.push_to_signal_buffer(0, sample.view(1, 1, 1))\n",
    "    #            speed = (t + 1) / (time.time() - start)\n",
    "    #            print(f'Generating: {t+1}/{seq_len}, Speed: {speed:.2f} samples/sec')\n",
    "    #    self.buffer = None\n",
    "    #    return torch.stack(output).cpu().numpy()\n",
    "    \n",
    "    def save_model(self, save_model_path:str):\n",
    "        try:\n",
    "            torch.save(self.state_dict(), save_model_path)\n",
    "            # torch.save(self, save_model_path)  # * this fails when data parallel\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "    def load_model(self, model_file_path:str):\n",
    "        try:\n",
    "            self.load_state_dict(\n",
    "                torch.load(model_file_path, map_location=lambda storage, loc: storage))\n",
    "            # torch.load(model_file_path)  # * this fails if trained on multiple GPU. use state dict.\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model settings:  {'lr': 0.0001, 'epoch': 1, 'iter': 7615, 'n_steps': 60000, 'seq_len': 5000, 'n_channels': 256, 'n_depth': 11, 'n_classes': 256, 'receptive_field_size': 2048}\n",
      "model path:  /diskB/6/out/models/fftnet/channels256_depth11_class256_rfsize2048_lr0.0001_loss1.355_nsteps60000_seqlen5000_iter1800000_index19\n",
      "loss ave:  1.3552132371803582\n",
      "_id mlmodeldic-8bf0f5f7-3524-4b49-877c-c1de97b83d46\n",
      "created 2018-12-14 01:30:29.604000\n",
      "Receptive Field: 2048 samples\n",
      "Trainable Parameters: 2.108 million\n"
     ]
    }
   ],
   "source": [
    "disable_cuda = True\n",
    "\n",
    "# load model\n",
    "#best_model_dic = MLModelDic.best_model('fftnet', key='loss_average', lowest=True)\n",
    "best_model_dic = MLModelDic.findOne({'_id': 'mlmodeldic-8bf0f5f7-3524-4b49-877c-c1de97b83d46'})\n",
    "#best_model_dic.remove_model()\n",
    "settings = best_model_dic.settings\n",
    "model_path = best_model_dic.save_model_path\n",
    "print('best model settings: ',settings)\n",
    "print('model path: ',model_path)\n",
    "print('loss ave: ',best_model_dic.loss_average)\n",
    "print('_id', best_model_dic._id)\n",
    "print('created', best_model_dic.created)\n",
    "model = FFTNet(n_channels=settings['n_channels'], n_depth=settings['n_depth'], n_classes=settings['n_classes'], disable_cuda=disable_cuda)\n",
    "model.load_model(model_path)\n",
    "model.eval()  # without droput\n",
    "if not disable_cuda:\n",
    "    model.cuda()\n",
    "else:\n",
    "    model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-2-3190f5e55ef0>\u001b[0m(26)\u001b[0;36menqueue\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     24 \u001b[0;31m        \"\"\"\n",
      "\u001b[0m\u001b[0;32m     25 \u001b[0;31m        \u001b[0mPdb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 26 \u001b[0;31m        \u001b[0msample_to_pop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m  \u001b[0;31m# get last\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     27 \u001b[0;31m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# [0, 1, 2, 3, .., y, z] -> [1, 2, 3, .., y, z, z]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     28 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0msample_to_pop\u001b[0m  \u001b[0;31m# return the last\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "> \u001b[0;32m<ipython-input-2-3190f5e55ef0>\u001b[0m(27)\u001b[0;36menqueue\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     24 \u001b[0;31m        \"\"\"\n",
      "\u001b[0m\u001b[0;32m     25 \u001b[0;31m        \u001b[0mPdb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     26 \u001b[0;31m        \u001b[0msample_to_pop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m  \u001b[0;31m# get last\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 27 \u001b[0;31m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# [0, 1, 2, 3, .., y, z] -> [1, 2, 3, .., y, z, z]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     28 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0msample_to_pop\u001b[0m  \u001b[0;31m# return the last\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> c\n",
      "> \u001b[0;32m<ipython-input-2-3190f5e55ef0>\u001b[0m(26)\u001b[0;36menqueue\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     24 \u001b[0;31m        \"\"\"\n",
      "\u001b[0m\u001b[0;32m     25 \u001b[0;31m        \u001b[0mPdb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 26 \u001b[0;31m        \u001b[0msample_to_pop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m  \u001b[0;31m# get last\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     27 \u001b[0;31m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# [0, 1, 2, 3, .., y, z] -> [1, 2, 3, .., y, z, z]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     28 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0msample_to_pop\u001b[0m  \u001b[0;31m# return the last\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> c\n",
      "> \u001b[0;32m<ipython-input-2-3190f5e55ef0>\u001b[0m(26)\u001b[0;36menqueue\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     24 \u001b[0;31m        \"\"\"\n",
      "\u001b[0m\u001b[0;32m     25 \u001b[0;31m        \u001b[0mPdb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 26 \u001b[0;31m        \u001b[0msample_to_pop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m  \u001b[0;31m# get last\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     27 \u001b[0;31m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# [0, 1, 2, 3, .., y, z] -> [1, 2, 3, .., y, z, z]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     28 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0msample_to_pop\u001b[0m  \u001b[0;31m# return the last\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sample_rate = 16000\n",
    "duration_ms = 100\n",
    "seq_len = int(sample_rate / (duration_ms/1000))\n",
    "model.cpu()\n",
    "# TODO: test data from dataset\n",
    "#output = model.generate(duration_ms=duration_ms, phonemes=[1]*seq_len, pitches=[200]*seq_len, sample_rate=sample_rate)\n",
    "output = model.generate(duration_ms=duration_ms, phonemes=None, pitches=None, sample_rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FFTNet(n_channels=N_CHANNELS, disable_cuda=DISABLE_CUDA)\n",
    "if not DISABLE_CUDA:\n",
    "    model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FFTNet(n_channels=N_CHANNELS, disable_cuda=DISABLE_CUDA)\n",
    "if not DISABLE_CUDA:\n",
    "    model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FFTNet(n_channels=N_CHANNELS, disable_cuda=DISABLE_CUDA)\n",
    "if not DISABLE_CUDA:\n",
    "    model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
